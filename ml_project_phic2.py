# -*- coding: utf-8 -*-
"""ML_project_PhiC2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tg2OEzzJMdRP6h9RUKITYa6is7VkjNpP

# Pip install
"""

!pip install Bio

!pip install wget

!pip install tensorflow-addons

!pip install -U tensorboard_plugin_profile

!pip install tensorflow_io

from datetime import datetime
import numpy as np
import re
import matplotlib.pyplot as plt 
import pandas as pd

# import BioPython modules

import Bio
from Bio import SeqIO
from Bio.Seq import Seq
from Bio import GenBank 
from Bio.SeqIO.FastaIO import SimpleFastaParser
from Bio import Entrez


# In[4]:


# Import PHASTER modules

import requests
import wget
import webbrowser
import json


# In[5]:


# Import BLAST modules

from Bio.Blast import NCBIWWW
from Bio.Blast import NCBIXML
from Bio.Blast.Applications import NcbiblastxCommandline


# In[6]:


# Import  machine learning modules

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import tensorflow_addons as tfa
import tensorflow_io as tfio

from keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau

# printing the version of tensorflow
print("TensorFlow version: ", tf.__version__)

"""# Import Files"""

# define function to import files and make a format that can easily be used

def convert_to_dataframe(file):
    '''
    
    '''
    # 1. pieces is the total length of the list/2
    pieces = len(file)/2
    # 2. split the list into format of [consensus number, code] 
    fasta_file_split = np.array_split(file, pieces)
    # 3. make a dataframe
    df = pd.DataFrame(fasta_file_split, 
                      columns=('Cluster', 'Code'))
    
    return df

def import_fasta_files(file):
    '''
    
    '''
    # create empty list to paste the fasta data into
    fasta_data = []
    with open(file, 'r') as fasta_file:
        fasta_data = fasta_file.read()
        fasta_data = fasta_data.strip().split()
    
    # apply dataframe function to the imported fasta file
    fasta_dataframe = convert_to_dataframe(fasta_data)
    return fasta_dataframe

# combine the clusters

from functools import reduce

def combine_cluster(y, n):
  y['combined'] = y.apply(lambda x: list([y['Code'][0]]), axis=1)
  y = y['combined'][0]

  y[0:n] = [reduce(lambda i, j: i+j, y[0:n])]

  return y

"""## The files"""

D133 = import_fasta_files('D133.fasta')
display(D133)

D133_dna = combine_cluster(D133, 6)
D133_dna

E011 = import_fasta_files('E011.fasta')
display(E011)

E011_dna = combine_cluster(E011, 5)
E011_dna

E185B = import_fasta_files('E185B.fasta')
display(E185B)

S4_1 = import_fasta_files('S4-1_genome.fasta')
display(S4_1)

S8 = import_fasta_files('S8_genome.fasta')
display(S8)

E185B_dna = E185B['Code'][0]

"""# One hot encoding"""

from sklearn.preprocessing import OneHotEncoder

def one_hot_encoder(sequence):
    encoder = OneHotEncoder()
    x = np.array(list(sequence)).reshape(-1, 1)
    x_onehotencoder = encoder.fit_transform(x).toarray()

    print(x_onehotencoder.shape)

    return x_onehotencoder

# the x 

E185B_one = one_hot_encoder(E185B['Code'][0])
# remove the last element because it is not 100 and does not contain useful info
E185B_onehot = E185B_one[:-1]
print('---')
print('One hot encoding')
print(E185B_onehot[:10])
print('\n')
print('Nucleotide bases:')
print(E185B_dna[:10])
print('---')

"""# Split the data

## using one hot encoding
"""

# How many elements each
# list should have
n = 100
  
# using list comprehension
x_onehot = [E185B_onehot[i:i + n] for i in range(0, len(E185B_onehot), n)]
#print(x_onehot[:1])

print(len(x_onehot))

"""## using the raw nucleotide bases"""

# defining variable
E185B_dna = E185B['Code'][0]
# showing the dna
print(E185B_dna[:10])

# length of the dna
print(f'Length of DNA =', len(E185B['Code'][0]))

E185B_dna

# How many elements each
# list should have
n = 100
  
# using list comprehension
E185B_x = [E185B_dna[i:i + n] for i in range(0, len(E185B_dna), n)]
print(E185B_x[:10])

print(len(E185B_x))

"""# Label the data

## PhiC2
"""

# make corresponding binary classification for if the code contains phiC2 (1)

# make a label 0 for the length of the training set
phic2_labels = np.zeros(len(E185B_x))
print('---')
print('Phic2 labels (initially):')
print(phic2_labels[:10])
print('---')

# change the 0 to 1 for the regions with phiC2 in
# these indices were found using the below and the table in Project v2.py file
phic2_labels[11479:12159] = 1
phic2_labels[14586:14861] = 1
phic2_labels[17070:17631] = 1
phic2_labels[18958:19495] = 1
phic2_labels[25867:26000] = 1
phic2_labels[37339:37814] = 1

print('---')
print('Phic2 labels (modifications):')
print(phic2_labels[11475:11485])
print('---')

# one hot encode labels

E185Blabel_onehot = one_hot_encoder(phic2_labels)

print('---')
print('The y label for E185B')
print('\n')
print('using one hot encoding')
print(E185Blabel_onehot[:5])
print('\n')
print('without using one hot encoding')
print(phic2_labels[:5])
print('---')

"""## The confirmation code"""

# this code was beloww was used to confirm the phic2 regions to double-check calculations

str_match = [s for s in E185B_x if "ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA" in s]
print(str_match)
#TTTTATAAAAATTTTGGTATTATATATACATACTAAATAATATCAAATATACATAAAAG

indices = [i for i, s in enumerate(E185B_x) if 'ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA' in s]
indices

"""# test train split"""

# test train split
x_touse = x_onehot
y_touse = phic2_labels

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_touse, y_touse, 
                                                    test_size=0.3, 
                                                    shuffle=False) # no shuffle to conserve nucleotide position

print('---')
print('x train length')
print(len(x_train))
print('---')

print('---')
print('y train length')
print(len(y_train))
print('---')

print('---')
print('x test length')
print(len(x_test))
print('---')

print('---')
print('y test length')
print(len(x_test))
print('---')

# make variables 2D 

def variable_2d(lst, n):
  x = lst 
  x = np.array(x)
  return x

x_train = variable_2d(x_train, 29509)
y_train = variable_2d(y_train, 29509)
x_test = variable_2d(x_train, 12647)
y_test = variable_2d(y_train, 12647)

"""## Example of what the training and test datasets look like"""

print('---')
print('x train')
print(x_train[0])
print('---')

print('---')
print('y train')
print(y_train[0])
print('---')

print('---')
print('x test')
print(x_test[0])
print('---')

print('---')
print('y test')
print(y_test[0])
print('---')

"""# Balance

## Visualisation of imbalance
"""

v = phic2_labels.astype('float32')

count_phic2 = np.count_nonzero(v == 1)
count_phic2

count_non = np.count_nonzero(v == 0)
count_non

data = {'0': count_non, 
        '1': count_phic2}

count_df = pd.DataFrame(data, index=['Number'])
count_df = count_df.T
count_df

fig, axs = plt.subplots(figsize=(5,8))

axs.bar(count_df.index.values, 
        count_df['Number'],
        color=['lightblue', 'darkred'])

axs.set_xlabel('Classification',
               fontsize=12)

axs.set_ylabel('Number of Occurences',
               fontsize=12)

axs.set_yticks(np.arange(0, 50000, 5000))

plt.text(-0.15, 41000, '39495', fontsize=15)
plt.text(0.91, 3500, '2661', fontsize=15)

plt.show()

"""## Balancing"""

# reshape y so smote works
y_train.reshape(29509, 1)

# reshape x so smote works
nsamples, nx, ny = x_train.shape
d2_train_dataset = x_train.reshape((nsamples,nx*ny))

d2_train_dataset

import imblearn

from imblearn.over_sampling import RandomOverSampler

y_train = np.array(y_train).reshape(29509, 1)

ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_resample(d2_train_dataset, y_train)

from imblearn.over_sampling import SMOTE

y_train = np.array(y_train)

oversample = SMOTE(random_state=42)
x_train_SMOTE, y_train_SMOTE = oversample.fit_resample(d2_train_dataset, y_train)

x_train_SMOTE = x_train_SMOTE.reshape(54646, 100, 4)

"""## Balanced Visualisation"""

table_smote = y_train_SMOTE.astype('float32')

smote_phic2 = np.count_nonzero(y_train_SMOTE == 1)
smote_phic2

smote_non = np.count_nonzero(table_smote == 0)
smote_non

data = {'0': smote_non, 
        '1': smote_phic2}

smote_df = pd.DataFrame(data, index=['Number'])
smote_df = smote_df.T
smote_df

fig, axs1 = plt.subplots(1, figsize=(12,8))

axs1.bar(smote_df.index.values, 
         smote_df['Number'],
         color=['lightblue', 'darkred'])

axs1.set_xlabel('Classification',
               fontsize=10)

axs1.set_ylabel('Number of Occurences',
               fontsize=10)

axs1.set_yticks(np.arange(0, 40000, 5000))

plt.text(-0.08, 29000, '27323', fontsize=15)
plt.text(0.94, 29000, '27323', fontsize=15)


plt.show()

"""# 1D CNN + SMOTE

using the same as before
"""

x_train_SMOTE = np.array(x_train_SMOTE)
print(x_train_SMOTE.shape)
y_train_SMOTE = y_train_SMOTE.reshape(54646, 1)
print(y_train_SMOTE.shape)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam

from keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau

# create the model
model_smote = Sequential()

# layer 1
model_smote.add(Conv1D(32, 3, 
                 activation='relu', 
                 input_shape=(100, 4)))
model_smote.add(MaxPooling1D(2))
model_smote.add(Dropout(0.25))

# layer 2
model_smote.add(Conv1D(16, 3, 
                 activation='relu'))
model_smote.add(MaxPooling1D(2))
model_smote.add(Dropout(0.25))

# layer 3
model_smote.add(Flatten())

# layer 4
model_smote.add(Dense(16, 
                activation='relu'))
model_smote.add(Dropout(0.5))

# layer 5
model_smote.add(Dense(1, 
                activation='relu'))


# compile the model
model_smote.compile(Adam(lr=.0001),
              loss='binary_crossentropy', 
              metrics=['accuracy'])

# binary crossentropy
model_smote.summary()

# Create a TensorBoard callback
logs = "logs/" + datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,
                                                      histogram_freq=1,
                                                      profile_batch='0, 10')

# callbacks
earlystop = EarlyStopping(patience = 10,
                          monitor='loss') # default is val loss so changed because not monitoring this metric
learning_rate_reduction = ReduceLROnPlateau(monitor = 'accuracy',
                                            patience = 2,
                                            verbose = 1,
                                            factor = 0.5,
                                            min_lr = 0.00001)
callbacks = [earlystop, learning_rate_reduction]

history_SMOTE = model_smote.fit(x=x_train_SMOTE,
                          y=y_train_SMOTE, 
                          epochs=20,
                          callbacks=[callbacks, tensorboard_callback])

# a dictionary of the losses and metrics at each epoch
history_SMOTE.history 

# check the values available with:
history_SMOTE.history.keys()
history_SMOTE.history.values()

# storing history_unoptimised as a dataframe to download
history_SMOTE_df = pd.DataFrame(data=history_SMOTE.history.values(),
                          index=history_SMOTE.history.keys())

# downloading the dataframe 
history_SMOTE_df.to_csv('history_unoptimised_df.csv')

### plot the loss function and metric of model_unoptimised 

# define figure space and axis
fig1, (axs1, axs2) = plt.subplots(1, 2, 
                                  figsize=(10, 6))

# plot loss data on axs1 
axs1.plot(history_SMOTE.history['loss'],
          color='black',
          label='Loss')

# plot accuracy data on axs2
axs2.plot(history_SMOTE.history['accuracy'],
          color='black',
          label='Accuracy')


# x-axis 
axs1.set_xlabel('Epoch (#)')
axs2.set_xlabel('Epoch (#)')
axs1.set_xticks(np.arange(0, 25, 5))
axs2.set_xticks(np.arange(0, 25, 5))

# y-axis
axs1.set_ylabel('Loss')
axs2.set_ylabel('Accuracy')

# legend
axs1.legend()
axs2.legend()

# grid
axs1.grid(0.01)
axs2.grid(0.01)

# add overall title to figure1
#fig1.suptitle('Figure 1 - The loss and accuracy of Unoptimised Convolutional Model')

# show the figure
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# plot tensorflow board 

# Load the TensorBoard notebook extension.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# Launch TensorBoard and navigate to the Profile tab to view performance profile
# %tensorboard --logdir=logs

# Retrieve a batch of images from the test set
train_batch = x_train_SMOTE
label_batch = y_train_SMOTE
test = model_smote.predict_on_batch(train_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions_SMOTE = tf.nn.sigmoid(test)
predictions_SMOTE = tf.where(test < 0.5, 0, 1)

print('Predictions:\n', predictions_SMOTE.numpy())
print('Labels:\n', label_batch)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_train_SMOTE, predictions_SMOTE)
print('Accuracy: %.2f%%' % (accuracy * 100.0))

from sklearn.metrics import roc_curve, auc

# apply AUC to train

train_y_pred_smote = model_smote.predict(x_train_SMOTE)

nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(y_train_SMOTE, train_y_pred_smote)
auc_keras = auc(nn_fpr_keras, nn_tpr_keras)
print('---')
print('AUC:')
print(auc_keras)
print('---')

# make a plot of ROC curve
plt.plot(nn_fpr_keras, nn_tpr_keras, 
         marker='.', 
         label='Neural Network (auc = %0.3f)' % auc_keras,
         color='red')

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

# Build confusion matrix from ground truth labels and model predictions
conf_mat = confusion_matrix(y_true=y_train_SMOTE, 
                            y_pred=predictions_SMOTE)
print('Confusion matrix:\n', conf_mat)
# Plot matrix
plt.matshow(conf_mat)
plt.colorbar()
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

"""### Apply Test data to SMOTE CNN"""

from sklearn.metrics import accuracy_score
from scipy.stats import itemfreq

# Predict results of test data
y_pred_raw = model_smote.predict(x_test)

y_predictions_SMOTE = tf.nn.sigmoid(y_pred_raw)
y_predictions_SMOTE = tf.where(y_pred_raw < 0.5, 0, 1)

# Get accuracy score
accuracy = accuracy_score(y_test, y_predictions_SMOTE)
print('Accuracy: %.2f%%' % (accuracy * 100.0))
# Show predicted classes
#print(itemfreq(y_pred_raw))

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

# Build confusion matrix from ground truth labels and model predictions
conf_mat = confusion_matrix(y_true=y_test, 
                            y_pred=y_predictions_SMOTE)
print('Confusion matrix:\n', conf_mat)
# Plot matrix
plt.matshow(conf_mat)
plt.colorbar()
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

from sklearn.metrics import roc_curve, auc

# apply AUC to test

y_pred_smote = model_smote.predict(x_test).ravel()

nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(y_test, y_pred_smote)
auc_keras = auc(nn_fpr_keras, nn_tpr_keras)
print('---')
print('AUC:')
print(auc_keras)
print('---')

# make a plot of ROC curve
plt.plot(nn_fpr_keras, nn_tpr_keras, 
         marker='.', 
         label='Neural Network (auc = %0.3f)' % auc_keras,
         color='red')

"""# Test 1D SMOTE CNN on S8 genome

why? because this is the other significant genome dataframe

## One Hot Encoder
"""

# the x 

S8_onehot = one_hot_encoder(S8['Code'][0])

print('---')
print('S8')
print('---')
print('One hot encoding')
print(S8_onehot[:10])
print('\n')
print('Nucleotide bases:')
print(S8['Code'][0][:10])
print('---')

"""## Split the data

### One Hot
"""

# How many elements each
# list should have
n = 100
  
# using list comprehension
S8_onehotsplit = [S8_onehot[i:i + n] for i in range(0, len(S8_onehot), n)]

print(len(S8_onehotsplit))

"""### Raw Nucleotide"""

# defining variable
S8_dna = S8['Code'][0]
# showing the dna
print(S8_dna[:10])

# length of the dna
print(f'Length of DNA =', len(S8['Code'][0]))

# How many elements each
# list should have
n = 100
  
# using list comprehension
S8_x = [S8_dna[i:i + n] for i in range(0, len(S8_dna), n)]
print(S8_x[:10])

print(len(S8_x))

"""## PhiC2 label"""

# make corresponding binary classification for if the code contains Tn6215 (1)

# make a label 0 for the length of the training set
phic2_S8_labels = np.zeros(len(S8_x))
print('---')
print('PhiC2 S8 labels:')
print(phic2_S8_labels[:10])
print('---')

# change the 0 to 1 for the regions with phiC2 in
# these indices were found using the below and the table in Project v2.py file
phic2_S8_labels[2824:2961] = 1
phic2_S8_labels[15369:15644] = 1
phic2_S8_labels[21093:21199] = 1
phic2_S8_labels[25837:25929] = 1
phic2_S8_labels[33271:34018] = 1

# one hot encode labels

S8label_onehot = one_hot_encoder(phic2_S8_labels)

print('---')
print('The y label for S8')
print('\n')
print('using one hot encoding')
print(S8label_onehot[:5])
print('\n')
print('without using one hot encoding')
print(phic2_S8_labels[:5])
print('---')

"""## Test the model"""

from sklearn.metrics import accuracy_score
from scipy.stats import itemfreq

x, y = S8_onehotsplit, phic2_S8_labels

S8_x_train, S8_x_test, S8_y_train, S8_y_test = train_test_split(x, y, 
                                                    test_size=0.05, 
                                                    shuffle=False)

# can make test really small because it doesnt matter if the end is chopped off because
# it only has 1 region of tn6215 in the middle

S8_x = variable_2d(S8_x_train, len(S8_x_train))
S8_y = variable_2d(S8_y_train, len(S8_x_train))

# Predict results of test data
S8_y_pred_raw = model_smote.predict(S8_x)

S8_y_predictions_SMOTE = tf.nn.sigmoid(S8_y_pred_raw)
S8_y_predictions_SMOTE = tf.where(S8_y_pred_raw < 0.5, 0, 1)

# Get accuracy score
accuracy = accuracy_score(S8_y, S8_y_predictions_SMOTE)
print('Accuracy: %.2f%%' % (accuracy * 100.0))
# Show predicted classes
#print(itemfreq(y_pred_raw))

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt

# Build confusion matrix from ground truth labels and model predictions
conf_mat = confusion_matrix(y_true=S8_y, 
                            y_pred=S8_y_predictions_SMOTE)
print('Confusion matrix:\n', conf_mat)
# Plot matrix
plt.matshow(conf_mat)
plt.colorbar()
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

from sklearn.metrics import roc_curve, auc

# apply AUC to test

S8_y_pred_smote = model_smote.predict(S8_x).ravel()

nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(S8_y, S8_y_pred_smote)
auc_keras = auc(nn_fpr_keras, nn_tpr_keras)
print('---')
print('AUC:')
print(auc_keras)
print('---')

# make a plot of ROC curve
plt.plot(nn_fpr_keras, nn_tpr_keras, 
         marker='.', 
         label='Neural Network (auc = %0.3f)' % auc_keras,
         color='red')

"""---"""