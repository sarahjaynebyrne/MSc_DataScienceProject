# -*- coding: utf-8 -*-
"""ML_project_PhiC2unbalanced.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nEiXt2DsDzt0v2sfSTGkd05_thKLI5Yj
"""



"""# Pip install"""

!pip install Bio

!pip install wget

!pip install tensorflow-addons

!pip install -U tensorboard_plugin_profile

!pip install tensorflow_io

from datetime import datetime
import numpy as np
import re
import matplotlib.pyplot as plt 
import pandas as pd

# import BioPython modules

import Bio
from Bio import SeqIO
from Bio.Seq import Seq
from Bio import GenBank 
from Bio.SeqIO.FastaIO import SimpleFastaParser
from Bio import Entrez


# In[4]:


# Import PHASTER modules

import requests
import wget
import webbrowser
import json


# In[5]:


# Import BLAST modules

from Bio.Blast import NCBIWWW
from Bio.Blast import NCBIXML
from Bio.Blast.Applications import NcbiblastxCommandline


# In[6]:


# Import  machine learning modules

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import tensorflow_addons as tfa
import tensorflow_io as tfio

from keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau

# printing the version of tensorflow
print("TensorFlow version: ", tf.__version__)

"""# Import Files"""

# define function to import files and make a format that can easily be used

def convert_to_dataframe(file):
    '''
    
    '''
    # 1. pieces is the total length of the list/2
    pieces = len(file)/2
    # 2. split the list into format of [consensus number, code] 
    fasta_file_split = np.array_split(file, pieces)
    # 3. make a dataframe
    df = pd.DataFrame(fasta_file_split, 
                      columns=('Cluster', 'Code'))
    
    return df

def import_fasta_files(file):
    '''
    
    '''
    # create empty list to paste the fasta data into
    fasta_data = []
    with open(file, 'r') as fasta_file:
        fasta_data = fasta_file.read()
        fasta_data = fasta_data.strip().split()
    
    # apply dataframe function to the imported fasta file
    fasta_dataframe = convert_to_dataframe(fasta_data)
    return fasta_dataframe

# combine the clusters

from functools import reduce

def combine_cluster(y, n):
  y['combined'] = y.apply(lambda x: list([y['Code'][0]]), axis=1)
  y = y['combined'][0]

  y[0:n] = [reduce(lambda i, j: i+j, y[0:n])]

  return y

"""## The files"""

D133 = import_fasta_files('D133.fasta')
display(D133)

D133_dna = combine_cluster(D133, 6)
D133_dna

E011 = import_fasta_files('E011.fasta')
display(E011)

E011_dna = combine_cluster(E011, 5)
E011_dna

E185B = import_fasta_files('E185B.fasta')
display(E185B)

S4_1 = import_fasta_files('S4-1_genome.fasta')
display(S4_1)

S8 = import_fasta_files('S8_genome.fasta')
display(S8)

E185B_dna = E185B['Code'][0]

"""# One hot encoding"""

from sklearn.preprocessing import OneHotEncoder

def one_hot_encoder(sequence):
    encoder = OneHotEncoder()
    x = np.array(list(sequence)).reshape(-1, 1)
    x_onehotencoder = encoder.fit_transform(x).toarray()

    print(x_onehotencoder.shape)

    return x_onehotencoder

# the x 

E185B_one = one_hot_encoder(E185B['Code'][0])
# remove the last element because it is not 100 and does not contain useful info
E185B_onehot = E185B_one[:-1]
print('---')
print('One hot encoding')
print(E185B_onehot[:10])
print('\n')
print('Nucleotide bases:')
print(E185B_dna[:10])
print('---')

"""# Split the data

## using one hot encoding
"""

# How many elements each
# list should have
n = 100
  
# using list comprehension
x_onehot = [E185B_onehot[i:i + n] for i in range(0, len(E185B_onehot), n)]
#print(x_onehot[:1])

print(len(x_onehot))

"""## using the raw nucleotide bases"""

# defining variable
E185B_dna = E185B['Code'][0]
# showing the dna
print(E185B_dna[:10])

# length of the dna
print(f'Length of DNA =', len(E185B['Code'][0]))

E185B_dna

# How many elements each
# list should have
n = 100
  
# using list comprehension
E185B_x = [E185B_dna[i:i + n] for i in range(0, len(E185B_dna), n)]
print(E185B_x[:10])

print(len(E185B_x))

"""# Label the data

## PhiC2
"""

# make corresponding binary classification for if the code contains phiC2 (1)

# make a label 0 for the length of the training set
phic2_labels = np.zeros(len(E185B_x))
print('---')
print('Phic2 labels (initially):')
print(phic2_labels[:10])
print('---')

# change the 0 to 1 for the regions with phiC2 in
# these indices were found using the below and the table in Project v2.py file
phic2_labels[11479:12159] = 1
phic2_labels[14586:14861] = 1
phic2_labels[17070:17631] = 1
phic2_labels[18958:19495] = 1
phic2_labels[25867:26000] = 1
phic2_labels[37339:37814] = 1

print('---')
print('Phic2 labels (modifications):')
print(phic2_labels[11475:11485])
print('---')

# one hot encode labels

E185Blabel_onehot = one_hot_encoder(phic2_labels)

print('---')
print('The y label for E185B')
print('\n')
print('using one hot encoding')
print(E185Blabel_onehot[:5])
print('\n')
print('without using one hot encoding')
print(phic2_labels[:5])
print('---')

"""## The confirmation code"""

# this code was beloww was used to confirm the phic2 regions to double-check calculations

str_match = [s for s in E185B_x if "ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA" in s]
print(str_match)
#TTTTATAAAAATTTTGGTATTATATATACATACTAAATAATATCAAATATACATAAAAG

indices = [i for i, s in enumerate(E185B_x) if 'ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA' in s]
indices

"""# test train split"""

# test train split
x_touse = x_onehot
y_touse = phic2_labels

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_touse, y_touse, 
                                                    test_size=0.3, 
                                                    shuffle=False) # no shuffle to conserve nucleotide position

print('---')
print('x train length')
print(len(x_train))
print('---')

print('---')
print('y train length')
print(len(y_train))
print('---')

print('---')
print('x test length')
print(len(x_test))
print('---')

print('---')
print('y test length')
print(len(x_test))
print('---')

# make variables 2D 

def variable_2d(lst, n):
  x = lst 
  x = np.array(x)
  return x

x_train = variable_2d(x_train, 29509)
y_train = variable_2d(y_train, 29509)
x_test = variable_2d(x_train, 12647)
y_test = variable_2d(y_train, 12647)

"""## Example of what the training and test datasets look like"""

print('---')
print('x train')
print(x_train[0])
print('---')

print('---')
print('y train')
print(y_train[0])
print('---')

print('---')
print('x test')
print(x_test[0])
print('---')

print('---')
print('y test')
print(y_test[0])
print('---')

"""# 1D CNN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam

# create the model
model = Sequential()
model.add(Conv1D(32, 3, 
                 activation='relu', 
                 input_shape=(100, 4)))
model.add(MaxPooling1D(2))
model.add(Conv1D(16, 3, 
                 activation='relu'))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(16, 
                activation='relu'))
model.add(Dense(1, 
                activation='relu'))

model.compile(Adam(lr=.0001),
              loss='binary_crossentropy', 
              metrics=['accuracy'])
# binary crossentropy
model.summary()

# Create a TensorBoard callback
logs = "logs/" + datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,
                                                      histogram_freq=1,
                                                      profile_batch='0, 10')

### Use the .evaluate method to check the initial loss and accuracy of the model
### on the test dataset
model.evaluate(x_test, y_test)

history = model.fit(x=x_train,
                    y=y_train, 
                    epochs=20,
                    callbacks=[tensorboard_callback])

# a dictionary of the losses and metrics at each epoch
history.history 

# check the values available with:
history.history.keys()
history.history.values()

# storing history_unoptimised as a dataframe to download
history_df = pd.DataFrame(data=history.history.values(),
                          index=history.history.keys())

# downloading the dataframe 
history_df.to_csv('history_unoptimised_df.csv')

### plot the loss function and metric of model_unoptimised 

# define figure space and axis
fig1, (axs1, axs2) = plt.subplots(1, 2, 
                                  figsize=(10, 6))

# plot loss data on axs1 
axs1.plot(history.history['loss'],
          color='hotpink',
          label='Loss')

# plot accuracy data on axs2
axs2.plot(history.history['accuracy'],
          color='magenta',
          label='Accuracy')


# x-axis 
axs1.set_xlabel('Epoch (#)')
axs2.set_xlabel('Epoch (#)')
axs1.set_xticks(np.arange(0, 25, 5))
axs2.set_xticks(np.arange(0, 25, 5))

# y-axis
axs1.set_ylabel('Loss')
axs2.set_ylabel('Accuracy')

# legend
axs1.legend()
axs2.legend()

# grid
axs1.grid(0.01)
axs2.grid(0.01)

# add overall title to figure1
fig1.suptitle('Figure 1 - The loss and accuracy of Unoptimised Convolutional Model')

# show the figure
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# plot tensorflow board 

# Load the TensorBoard notebook extension.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# Launch TensorBoard and navigate to the Profile tab to view performance profile
# %tensorboard --logdir=logs

# Retrieve a batch of images from the test set
train_batch = x_train
label_batch = y_train
test = model.predict_on_batch(train_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions_unoptimised = tf.nn.sigmoid(test)
predictions_unoptimised = tf.where(test < 0.5, 0, 1)

print('Predictions:\n', predictions_unoptimised.numpy())
print('Labels:\n', label_batch)

from sklearn.metrics import accuracy_score
from scipy.stats import itemfreq

# Predict results of test data
y_pred_raw = model.predict(x_test)

# Get accuracy score
accuracy = accuracy_score(y_test, y_pred_raw)
print('Accuracy: %.2f%%' % (accuracy * 100.0))

# Show predicted classes
print(itemfreq(y_pred_raw))

"""92.5% accuracy because it is inbalanced data and is just marking everything in the test as 0 so thats why the accuracy is still high.

As we can see, the high accuracy rate is a very misleading metric.

A better way to evaluate the results is through the use of a confusion matrix. In a confusion matrix the number of correct and incorrect predictions are summarised with count values and broken down by each class. Showing what each target was predicted to be by the classifier, with high diagonal values showing a high number of correct predictions across all classes.

"""

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
# Build confusion matrix from ground truth labels and model predictions
conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_raw)
print('Confusion matrix:\n', conf_mat)
# Plot matrix
plt.matshow(conf_mat)
plt.colorbar()
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

"""Here we can clearly see that every prediction was for class = 0.

# Pip install
"""

!pip install Bio

!pip install wget

!pip install tensorflow-addons

!pip install -U tensorboard_plugin_profile

!pip install tensorflow_io

from datetime import datetime
import numpy as np
import re
import matplotlib.pyplot as plt 
import pandas as pd

# import BioPython modules

import Bio
from Bio import SeqIO
from Bio.Seq import Seq
from Bio import GenBank 
from Bio.SeqIO.FastaIO import SimpleFastaParser
from Bio import Entrez


# In[4]:


# Import PHASTER modules

import requests
import wget
import webbrowser
import json


# In[5]:


# Import BLAST modules

from Bio.Blast import NCBIWWW
from Bio.Blast import NCBIXML
from Bio.Blast.Applications import NcbiblastxCommandline


# In[6]:


# Import  machine learning modules

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import tensorflow_addons as tfa
import tensorflow_io as tfio

from keras.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau

# printing the version of tensorflow
print("TensorFlow version: ", tf.__version__)

"""# Import Files"""

# define function to import files and make a format that can easily be used

def convert_to_dataframe(file):
    '''
    
    '''
    # 1. pieces is the total length of the list/2
    pieces = len(file)/2
    # 2. split the list into format of [consensus number, code] 
    fasta_file_split = np.array_split(file, pieces)
    # 3. make a dataframe
    df = pd.DataFrame(fasta_file_split, 
                      columns=('Cluster', 'Code'))
    
    return df

def import_fasta_files(file):
    '''
    
    '''
    # create empty list to paste the fasta data into
    fasta_data = []
    with open(file, 'r') as fasta_file:
        fasta_data = fasta_file.read()
        fasta_data = fasta_data.strip().split()
    
    # apply dataframe function to the imported fasta file
    fasta_dataframe = convert_to_dataframe(fasta_data)
    return fasta_dataframe

# combine the clusters

from functools import reduce

def combine_cluster(y, n):
  y['combined'] = y.apply(lambda x: list([y['Code'][0]]), axis=1)
  y = y['combined'][0]

  y[0:n] = [reduce(lambda i, j: i+j, y[0:n])]

  return y

"""## The files"""

D133 = import_fasta_files('D133.fasta')
display(D133)

D133_dna = combine_cluster(D133, 6)
D133_dna

E011 = import_fasta_files('E011.fasta')
display(E011)

E011_dna = combine_cluster(E011, 5)
E011_dna

E185B = import_fasta_files('E185B.fasta')
display(E185B)

S4_1 = import_fasta_files('S4-1_genome.fasta')
display(S4_1)

S8 = import_fasta_files('S8_genome.fasta')
display(S8)

E185B_dna = E185B['Code'][0]

"""# One hot encoding"""

from sklearn.preprocessing import OneHotEncoder

def one_hot_encoder(sequence):
    encoder = OneHotEncoder()
    x = np.array(list(sequence)).reshape(-1, 1)
    x_onehotencoder = encoder.fit_transform(x).toarray()

    print(x_onehotencoder.shape)

    return x_onehotencoder

# the x 

E185B_one = one_hot_encoder(E185B['Code'][0])
# remove the last element because it is not 100 and does not contain useful info
E185B_onehot = E185B_one[:-1]
print('---')
print('One hot encoding')
print(E185B_onehot[:10])
print('\n')
print('Nucleotide bases:')
print(E185B_dna[:10])
print('---')

"""# Split the data

## using one hot encoding
"""

# How many elements each
# list should have
n = 100
  
# using list comprehension
x_onehot = [E185B_onehot[i:i + n] for i in range(0, len(E185B_onehot), n)]
#print(x_onehot[:1])

print(len(x_onehot))

"""## using the raw nucleotide bases"""

# defining variable
E185B_dna = E185B['Code'][0]
# showing the dna
print(E185B_dna[:10])

# length of the dna
print(f'Length of DNA =', len(E185B['Code'][0]))

E185B_dna

# How many elements each
# list should have
n = 100
  
# using list comprehension
E185B_x = [E185B_dna[i:i + n] for i in range(0, len(E185B_dna), n)]
print(E185B_x[:10])

print(len(E185B_x))

"""# Label the data

## PhiC2
"""

# make corresponding binary classification for if the code contains phiC2 (1)

# make a label 0 for the length of the training set
phic2_labels = np.zeros(len(E185B_x))
print('---')
print('Phic2 labels (initially):')
print(phic2_labels[:10])
print('---')

# change the 0 to 1 for the regions with phiC2 in
# these indices were found using the below and the table in Project v2.py file
phic2_labels[11479:12159] = 1
phic2_labels[14586:14861] = 1
phic2_labels[17070:17631] = 1
phic2_labels[18958:19495] = 1
phic2_labels[25867:26000] = 1
phic2_labels[37339:37814] = 1

print('---')
print('Phic2 labels (modifications):')
print(phic2_labels[11475:11485])
print('---')

# one hot encode labels

E185Blabel_onehot = one_hot_encoder(phic2_labels)

print('---')
print('The y label for E185B')
print('\n')
print('using one hot encoding')
print(E185Blabel_onehot[:5])
print('\n')
print('without using one hot encoding')
print(phic2_labels[:5])
print('---')

"""## The confirmation code"""

# this code was beloww was used to confirm the phic2 regions to double-check calculations

str_match = [s for s in E185B_x if "ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA" in s]
print(str_match)
#TTTTATAAAAATTTTGGTATTATATATACATACTAAATAATATCAAATATACATAAAAG

indices = [i for i, s in enumerate(E185B_x) if 'ACTTGAAATGTTTTTCAAAAGTAGTAAAATAATGGATAACTATACATACA' in s]
indices

"""# test train split"""

# test train split
x_touse = x_onehot
y_touse = phic2_labels

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_touse, y_touse, 
                                                    test_size=0.3, 
                                                    shuffle=False) # no shuffle to conserve nucleotide position

print('---')
print('x train length')
print(len(x_train))
print('---')

print('---')
print('y train length')
print(len(y_train))
print('---')

print('---')
print('x test length')
print(len(x_test))
print('---')

print('---')
print('y test length')
print(len(x_test))
print('---')

# make variables 2D 

def variable_2d(lst, n):
  x = lst 
  x = np.array(x)
  return x

x_train = variable_2d(x_train, 29509)
y_train = variable_2d(y_train, 29509)
x_test = variable_2d(x_train, 12647)
y_test = variable_2d(y_train, 12647)

"""## Example of what the training and test datasets look like"""

print('---')
print('x train')
print(x_train[0])
print('---')

print('---')
print('y train')
print(y_train[0])
print('---')

print('---')
print('x test')
print(x_test[0])
print('---')

print('---')
print('y test')
print(y_test[0])
print('---')

"""# 1D CNN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam

# create the model
model = Sequential()
model.add(Conv1D(32, 3, 
                 activation='relu', 
                 input_shape=(100, 4)))
model.add(MaxPooling1D(2))
model.add(Conv1D(16, 3, 
                 activation='relu'))
model.add(MaxPooling1D(2))
model.add(Flatten())
model.add(Dense(16, 
                activation='relu'))
model.add(Dense(1, 
                activation='relu'))

model.compile(Adam(lr=.0001),
              loss='binary_crossentropy', 
              metrics=['accuracy'])
# binary crossentropy
model.summary()

# Create a TensorBoard callback
logs = "logs/" + datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs,
                                                      histogram_freq=1,
                                                      profile_batch='0, 10')

### Use the .evaluate method to check the initial loss and accuracy of the model
### on the test dataset
model.evaluate(x_test, y_test)

history = model.fit(x=x_train,
                    y=y_train, 
                    epochs=20,
                    callbacks=[tensorboard_callback])

# a dictionary of the losses and metrics at each epoch
history.history 

# check the values available with:
history.history.keys()
history.history.values()

# storing history_unoptimised as a dataframe to download
history_df = pd.DataFrame(data=history.history.values(),
                          index=history.history.keys())

# downloading the dataframe 
history_df.to_csv('history_unoptimised_df.csv')

### plot the loss function and metric of model_unoptimised 

# define figure space and axis
fig1, (axs1, axs2) = plt.subplots(1, 2, 
                                  figsize=(10, 6))

# plot loss data on axs1 
axs1.plot(history.history['loss'],
          color='hotpink',
          label='Loss')

# plot accuracy data on axs2
axs2.plot(history.history['accuracy'],
          color='magenta',
          label='Accuracy')


# x-axis 
axs1.set_xlabel('Epoch (#)')
axs2.set_xlabel('Epoch (#)')
axs1.set_xticks(np.arange(0, 25, 5))
axs2.set_xticks(np.arange(0, 25, 5))

# y-axis
axs1.set_ylabel('Loss')
axs2.set_ylabel('Accuracy')

# legend
axs1.legend()
axs2.legend()

# grid
axs1.grid(0.01)
axs2.grid(0.01)

# add overall title to figure1
fig1.suptitle('Figure 1 - The loss and accuracy of Unoptimised Convolutional Model')

# show the figure
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# plot tensorflow board 

# Load the TensorBoard notebook extension.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# Launch TensorBoard and navigate to the Profile tab to view performance profile
# %tensorboard --logdir=logs

# Retrieve a batch of images from the test set
train_batch = x_train
label_batch = y_train
test = model.predict_on_batch(train_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions_unoptimised = tf.nn.sigmoid(test)
predictions_unoptimised = tf.where(test < 0.5, 0, 1)

print('Predictions:\n', predictions_unoptimised.numpy())
print('Labels:\n', label_batch)

from sklearn.metrics import accuracy_score
from scipy.stats import itemfreq

# Predict results of test data
y_pred_raw = model.predict(x_test)

# Get accuracy score
accuracy = accuracy_score(y_test, y_pred_raw)
print('Accuracy: %.2f%%' % (accuracy * 100.0))

# Show predicted classes
print(itemfreq(y_pred_raw))

"""92.5% accuracy because it is inbalanced data and is just marking everything in the test as 0 so thats why the accuracy is still high.

As we can see, the high accuracy rate is a very misleading metric.

A better way to evaluate the results is through the use of a confusion matrix. In a confusion matrix the number of correct and incorrect predictions are summarised with count values and broken down by each class. Showing what each target was predicted to be by the classifier, with high diagonal values showing a high number of correct predictions across all classes.

"""

from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
# Build confusion matrix from ground truth labels and model predictions
conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_raw)
print('Confusion matrix:\n', conf_mat)
# Plot matrix
plt.matshow(conf_mat)
plt.colorbar()
plt.ylabel('Real Class')
plt.xlabel('Predicted Class')
plt.show()

"""Here we can clearly see that every prediction was for class = 0."""